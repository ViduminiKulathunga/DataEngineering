{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17054ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7210cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92695d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/26 17:01:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.master(\"spark://192.168.2.122:7077\").appName(\"kulathunga-vidumini\").config(\"spark.dynamicAllocation.enabled\", True).config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True).config(\"spark.shuffle.service.enabled\", True).config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\").config(\"spark.cores.max\", 2).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3157f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d107f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark_context.textFile('hdfs://192.168.2.122:9000/europarl/parking-citations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ae99a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "|Ticket number|         Issue Date|Issue time|Meter Id|Marked Time|RP State Plate|Plate Expiry Date| VIN|Make|Body Style|Color|            Location|Route|Agency|Violation code|Violation Description|Fine amount| Latitude|Longitude|Agency Description|Color Description|Body Style Description|\n",
      "+-------------+-------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "|   1103341116|2015-12-21 00:00:00|    1251.0|    null|       null|            CA|         200304.0|null|HOND|        PA|   GY|     13147 WELBY WAY|01521|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1103700150|2015-12-21 00:00:00|    1435.0|    null|       null|            CA|         201512.0|null| GMC|        VN|   WH|       525 S MAIN ST| 1C51|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1104803000|2015-12-21 00:00:00|    2055.0|    null|       null|            CA|         201503.0|null|NISS|        PA|   BK|       200 WORLD WAY|  2R2|   2.0|          8939|           WHITE CURB|       58.0|6439997.9|1802686.4|              null|             null|                  null|\n",
      "|   1104820732|2015-12-26 00:00:00|    1515.0|    null|       null|            CA|             null|null|ACUR|        PA|   WH|       100 WORLD WAY| 2F11|   2.0|           000|               17104h|       null|6440041.1|1802686.2|              null|             null|                  null|\n",
      "|   1105461453|2015-09-15 00:00:00|     115.0|    null|       null|            CA|         200316.0|null|CHEV|        PA|   BK|  GEORGIA ST/OLYMPIC|1FB70|   1.0|         8069A| NO STOPPING/STANDING|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106226590|2015-09-15 00:00:00|      19.0|    null|       null|            CA|         201507.0|null|CHEV|        VN|   GY|  SAN PEDRO S/O BOYD|1A35W|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106500452|2015-12-17 00:00:00|    1710.0|    null|       null|            CA|         201605.0|null|MAZD|        PA|   BL|     SUNSET/ALVARADO|00217|   1.0|          8070| PARK IN GRID LOCK ZN|      163.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106500463|2015-12-17 00:00:00|    1710.0|    null|       null|            CA|         201602.0|null|TOYO|        PA|   BK|     SUNSET/ALVARADO|00217|   1.0|          8070| PARK IN GRID LOCK ZN|      163.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106506402|2015-12-22 00:00:00|     945.0|    null|       null|            CA|         201605.0|null|CHEV|        PA|   BR|      721 S WESTLAKE| 2A75|   1.0|        8069AA|     NO STOP/STAND AM|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106506413|2015-12-22 00:00:00|    1100.0|    null|       null|            CA|         201701.0|null|NISS|        PA|   SI|     1159 HUNTLEY DR| 2A75|   1.0|        8069AA|     NO STOP/STAND AM|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106506424|2015-12-22 00:00:00|    1100.0|    null|       null|            CA|         201511.0|null|FORD|        TR|   WH|     1159 HUNTLEY DR| 2A75|   1.0|        8069AA|     NO STOP/STAND AM|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106506435|2015-12-22 00:00:00|    1105.0|    null|       null|            CA|         201701.0|null|CHRY|        PA|   GO|     1159 HUNTLEY DR| 2A75|   1.0|        8069AA|     NO STOP/STAND AM|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106506446|2015-12-22 00:00:00|    1110.0|    null|       null|            CA|         201511.0|null| BMW|        PA|   BK|      1200 W MIRAMAR| 2A75|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1106549754|2015-12-15 00:00:00|     825.0|    null|       null|            CA|         201607.0|null|PTRB|        TR|   BK|           4TH/STATE| CM96|   1.0|         8069A| NO STOPPING/STANDING|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1107179581|2015-12-27 00:00:00|    1055.0|    null|       null|            CA|         201605.0|null|TOYO|        PA|   BK|3100 N HOLLYRIDGE DR| null|  54.0|         8058L|         PREF PARKING|       68.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1107179592|2015-12-27 00:00:00|    1200.0|    null|       null|            CA|         201602.0|null|MBNZ|        PA|   BK|   3115 N BERENDO DR| null|  54.0|         8058L|         PREF PARKING|       68.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1107179603|2015-12-27 00:00:00|    1400.0|    null|       null|            CA|         201611.0|null|NISS|        PA|   WH| 3100 N BEACHWOOD DR| null|  54.0|         8058L|         PREF PARKING|       68.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1107539823|2015-09-16 00:00:00|    2120.0|    null|       null|            CA|         201502.0|null|NISS|        PA| null|      BLAINE/11TH PL|1FB95|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1107539834|2015-09-16 00:00:00|    1045.0|    null|       null|            CA|             null|null|CHEV|        PA|   BK|  1246 S FIGUEROA ST| 1L20|   1.0|        8069AP|     NO STOP/STAND PM|       93.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "|   1107780811|2015-12-22 00:00:00|    1102.0|    null|       null|            CA|         201606.0|null|HOND|        PA|   BK|       PLATA/RAMPART|  2A1|   1.0|         8069B|           NO PARKING|       73.0|  99999.0|  99999.0|              null|             null|                  null|\n",
      "+-------------+-------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.csv(\"hdfs://192.168.2.122:9000/parking-citations.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# show the first 20 rows of the dataframe\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e81d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticket number: string (nullable = true)\n",
      " |-- Issue Date: timestamp (nullable = true)\n",
      " |-- Issue time: double (nullable = true)\n",
      " |-- Meter Id: string (nullable = true)\n",
      " |-- Marked Time: double (nullable = true)\n",
      " |-- RP State Plate: string (nullable = true)\n",
      " |-- Plate Expiry Date: double (nullable = true)\n",
      " |-- VIN: string (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Body Style: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Route: string (nullable = true)\n",
      " |-- Agency: double (nullable = true)\n",
      " |-- Violation code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- Fine amount: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Agency Description: string (nullable = true)\n",
      " |-- Color Description: string (nullable = true)\n",
      " |-- Body Style Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc5456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13077724"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9eaeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95896067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"VIN\", \"Latitude\", \"Longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6cddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "df = df.withColumn(\"Fine amount\", df[\"Fine amount\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c993a63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "maximum_value = df.agg({\"Fine amount\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d5e17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df[\"Fine amount\"] == maximum_value).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63a21599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Make|  count|\n",
      "+----+-------+\n",
      "|TOYT|2150768|\n",
      "|HOND|1479996|\n",
      "|FORD|1116235|\n",
      "|NISS| 945133|\n",
      "|CHEV| 892676|\n",
      "| BMW| 603092|\n",
      "|MERZ| 543298|\n",
      "|VOLK| 432030|\n",
      "|HYUN| 404917|\n",
      "|DODG| 391686|\n",
      "|LEXS| 368420|\n",
      "| KIA| 328155|\n",
      "|JEEP| 316300|\n",
      "|AUDI| 255395|\n",
      "|MAZD| 242344|\n",
      "|OTHR| 205546|\n",
      "| GMC| 184889|\n",
      "|INFI| 174315|\n",
      "|CHRY| 159948|\n",
      "|SUBA| 154640|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# group the rows by the values in the column and count the number of occurrences\n",
    "frequent_items = df.groupBy(\"Make\").count()\n",
    "\n",
    "# sort the resulting dataframe in descending order by the count and select the top 20 rows\n",
    "top_20 = frequent_items.orderBy(\"count\", ascending=False).limit(20)\n",
    "\n",
    "# print the top 20 frequent items to the console\n",
    "top_20.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeacbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the COLORS dictionary\n",
    "COLORS = {\n",
    "    'AL': 'Aluminum', 'AM': 'Amber', 'BG': 'Beige', 'BK': 'Black', 'BL': 'Blue',\n",
    "    'BN': 'Brown', 'BR': 'Brown', 'BZ': 'Bronze', 'CH': 'Charcoal', 'DK': 'Dark',\n",
    "    'GD': 'Gold', 'GO': 'Gold', 'GN': 'Green', 'GY': 'Gray', 'GT': 'Granite',\n",
    "    'IV': 'Ivory', 'LT': 'Light', 'OL': 'Olive', 'OR': 'Orange', 'MR': 'Maroon',\n",
    "    'PK': 'Pink', 'RD': 'Red', 'RE': 'Red', 'SI': 'Silver', 'SL': 'Silver',\n",
    "    'SM': 'Smoke', 'TN': 'Tan', 'VT': 'Violet', 'WT': 'White', 'WH': 'White',\n",
    "    'YL': 'Yellow', 'YE': 'Yellow', 'UN': 'Unknown'\n",
    "}\n",
    "\n",
    "# Define the UDF to map the colors\n",
    "def map_udf_colors(color):\n",
    "    return COLORS.get(color, color)\n",
    "\n",
    "# Register the UDF with Spark\n",
    "map_colors = udf(map_udf_colors, StringType())\n",
    "\n",
    "# Create a new column \"color_long\" in the dataframe\n",
    "df = df.withColumn(\"color_long\", map_colors(df.Color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9ea141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+----------+--------+-----------+--------------+-----------------+----+----------+-----+------------------+-----+------+--------------+---------------------+-----------+------------------+-----------------+----------------------+----------+\n",
      "|Ticket number|         Issue Date|Issue time|Meter Id|Marked Time|RP State Plate|Plate Expiry Date|Make|Body Style|Color|          Location|Route|Agency|Violation code|Violation Description|Fine amount|Agency Description|Color Description|Body Style Description|color_long|\n",
      "+-------------+-------------------+----------+--------+-----------+--------------+-----------------+----+----------+-----+------------------+-----+------+--------------+---------------------+-----------+------------------+-----------------+----------------------+----------+\n",
      "|   1103341116|2015-12-21 00:00:00|    1251.0|    null|       null|            CA|         200304.0|HOND|        PA|   GY|   13147 WELBY WAY|01521|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|              null|             null|                  null|      Gray|\n",
      "|   1103700150|2015-12-21 00:00:00|    1435.0|    null|       null|            CA|         201512.0| GMC|        VN|   WH|     525 S MAIN ST| 1C51|   1.0|        4000A1|   NO EVIDENCE OF REG|       50.0|              null|             null|                  null|     White|\n",
      "|   1104803000|2015-12-21 00:00:00|    2055.0|    null|       null|            CA|         201503.0|NISS|        PA|   BK|     200 WORLD WAY|  2R2|   2.0|          8939|           WHITE CURB|       58.0|              null|             null|                  null|     Black|\n",
      "|   1104820732|2015-12-26 00:00:00|    1515.0|    null|       null|            CA|             null|ACUR|        PA|   WH|     100 WORLD WAY| 2F11|   2.0|           000|               17104h|       null|              null|             null|                  null|     White|\n",
      "|   1105461453|2015-09-15 00:00:00|     115.0|    null|       null|            CA|         200316.0|CHEV|        PA|   BK|GEORGIA ST/OLYMPIC|1FB70|   1.0|         8069A| NO STOPPING/STANDING|       93.0|              null|             null|                  null|     Black|\n",
      "+-------------+-------------------+----------+--------+-----------+--------------+-----------------+----+----------+-----+------------------+-----+------+--------------+---------------------+-----------+------------------+-----------------+----------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "401fa955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gray'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Filter the dataframe to only include rows where \"Make\" is \"TOYT\"\n",
    "toyota = df.filter(df.Make == \"TOYT\")\n",
    "\n",
    "# Group the rows by \"color_long\" and count the number of occurrences of each color\n",
    "toyota_color_counts = toyota.groupBy(\"color_long\").count()\n",
    "\n",
    "# Sort the resulting dataframe in descending order by the count of each color\n",
    "sorted_count = toyota_color_counts.sort(desc(\"count\"))\n",
    "\n",
    "# Select the first row to get the most frequent color\n",
    "sorted_count.first()[\"color_long\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "847c194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a0bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
